{
 "awd_id": "2441531",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Developing Language Models via Bio-inspired Learning Algorithms",
 "cfda_num": "47.070, 47.075",
 "org_code": "05010000",
 "po_phone": "7032924748",
 "po_email": "sgage@nsf.gov",
 "po_sign_block_name": "Stephanie Gage",
 "awd_eff_date": "2025-06-15",
 "awd_exp_date": "2030-05-31",
 "tot_intn_awd_amt": 598499.0,
 "awd_amount": 598499.0,
 "awd_min_amd_letter_date": "2025-06-03",
 "awd_max_amd_letter_date": "2025-06-03",
 "awd_abstract_narration": "This project leverages recent findings from neuroscience to create artificial intelligence (AI) language models that store knowledge and respond to input more like biological brains. This will be done by changing the flow of information through the language model in ways that make it more responsive to human emotions, more skilled at remembering and using information provide by humans, and better able to make fair and equitable decisions when the desires of many people come into conflict. This is important because many harms caused by AI systems can be traced to over-reliance on training data and an inability to adapt to the situation and needs of specific individuals. The new models will be rigorously tested in simulations where humans and AIs work together to make decisions and accomplish tasks, and will be probed to determine the potential benefits and/or risks introduced by this biologically-inspired computing paradigm. Additionally, this research will expand participation in science and technology by involving undergraduate students including a visiting research program that remotely hosts students from other universities. Research results will be shared via workshops, academic articles, and public media.\r\n\r\nThis research will be conducted via three research thrusts, each addressing a specific biologically-inspired property that current language models lack, resulting in new open-source foundation models in the 7B-20B parameter range. Specifically, the research team will develop biologically inspired algorithms that emulate mirror neurons, long-term potentiation, and metaplasticity within transformer-based language models. The models created during this project will be evaluated in two ways: (a) via automated metrics that assess the emotional responsiveness and factual accuracy of the model, and (b) via direct human-large language model (LLM) interactions in multi-party scenarios where participants have conflicting priorities, and where the LLM has control over (low-risk) outcomes that affect humans. These studies are designed to preserve participant well-being while providing a valuable litmus test of language model behavior \u201cin the wild\u201d. It is anticipated that developed language models will be better able to manage contested resources, and more effective at responding appropriately to nuanced human emotions and experiences. They may also be uniquely suited to agentic scenarios that require the model to iteratively formulate objectives, plan actions, write and execute code, and deliver reasonable results back to humans in real-world scenarios with domain-specific constraints. \r\n\r\nThis project is jointly funded by the Foundations of Emerging Technologies Program, the Robust Intelligence Program, and the Science of Learning and Augmented Intelligence Program.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nancy",
   "pi_last_name": "Fulda",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nancy Fulda",
   "pi_email_addr": "nfulda@cs.byu.edu",
   "nsf_id": "000820814",
   "pi_start_date": "2025-06-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Brigham Young University",
  "inst_street_address": "A-153 ASB",
  "inst_street_address_2": "",
  "inst_city_name": "PROVO",
  "inst_state_code": "UT",
  "inst_state_name": "Utah",
  "inst_phone_num": "8014223360",
  "inst_zip_code": "846021128",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "UT03",
  "org_lgl_bus_name": "BRIGHAM YOUNG UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JWSYC7RUMJD1"
 },
 "perf_inst": {
  "perf_inst_name": "Brigham Young University",
  "perf_str_addr": "A-153 ASB",
  "perf_city_name": "PROVO",
  "perf_st_code": "UT",
  "perf_st_name": "Utah",
  "perf_zip_code": "846021128",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "UT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "089Y00",
   "pgm_ele_name": "FET-Fndtns of Emerging Tech"
  },
  {
   "pgm_ele_code": "127Y00",
   "pgm_ele_name": "Sci of Lrng & Augmented Intel"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "059Z",
   "pgm_ref_txt": "Science of Learning"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7931",
   "pgm_ref_txt": "COMPUTATIONAL BIOLOGY"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 598499.0
  }
 ],
 "por": null
}