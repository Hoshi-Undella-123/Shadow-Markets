{
 "awd_id": "2526533",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "I-Corps: Translation Potential of Resilient, Flexible, and Intuitive Situational Awareness via Next-Generation Artificial Intelligence (AI) and Flexible Middleware",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": "7032922160",
 "po_email": "rshuman@nsf.gov",
 "po_sign_block_name": "Ruth Shuman",
 "awd_eff_date": "2025-07-01",
 "awd_exp_date": "2026-06-30",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2025-06-23",
 "awd_max_amd_letter_date": "2025-06-23",
 "awd_abstract_narration": "This I-Corps project is based on the development of a software platform to provide a wide range of computer vision tasks with minimal computational effort for mobile applications, including autonomous vehicles, robotics, and traffic monitoring. Currently, traditional models using artificial intelligence (AI) solutions are static, monolithic, and optimized for worst-case scenarios, leading to significant computational demands that mobile devices often cannot meet. This technology provides operators with real-time, actionable insights, even in low-bandwidth environments. The solution is based on a new generation of AI algorithms, empowering systems with new analysis capabilities. In addition, it uses a modular approach that is designed to integrate seamlessly with existing systems, which may ensure greater scalability and adaptability for rapidly evolving missions. The ability of the platform to deploy AI algorithms in a flexible, resilient, and distributed fashion may make it possible to gather information from multiple sources from the environment, understand what the information means, and use the information to predict what may happen next, referred to as dynamic situational awareness. This technology may improve the decision making and safety of first responders and workers across fields and commercial areas and improve U.S. defense capabilities.\r\n\r\nThis I-Corps project utilizes experiential learning coupled with first-hand investigation of the industry ecosystem to assess the translation potential of a software platform to provide a wide range of computer vision tasks by leveraging adaptive sensor fusion and edge processing. Computer vision is the cornerstone of a broad range of current and future mobile applications, such as vehicular autonomy, robotics, and traffic monitoring. In this domain, deep neural networks (DNN) achieve the best performance, where many generations of models have been designed to accomplish various tasks. Currently, these algorithms are designed for execution on a single machine and are static and monolithic. One downside of this approach is that the algorithms are tuned to process the \u201cworst\u201d case input, and have computational demands often failing to meet application requirements when considering the limitations of mobile devices. Moreover, offloading the execution to compute-capable devices (edge computing) requires the transfer of information-rich signals (e.g., camera, light detection and ranging (LiDAR) and radar imaging) over capacity constrained and volatile wireless channels. The goal is to provide operators with real-time, actionable insights, even in low-bandwidth or contested environments. The ability of the platform to deploy AI algorithms in a flexible, resilient and distributed fashion may make it possible to provide dynamic situational awareness in a broad range of applications. This may bring a new generation of AI algorithms to the market, empowering vehicles with new analysis capabilities that may improve decision making and safety.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Marco",
   "pi_last_name": "Levorato",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Marco Levorato",
   "pi_email_addr": "levorato@uci.edu",
   "nsf_id": "000658096",
   "pi_start_date": "2025-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Irvine",
  "inst_street_address": "160 ALDRICH HALL",
  "inst_street_address_2": "",
  "inst_city_name": "IRVINE",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "9498247295",
  "inst_zip_code": "926970001",
  "inst_country_name": "United States",
  "cong_dist_code": "47",
  "st_cong_dist_code": "CA47",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA IRVINE",
  "org_prnt_uei_num": "MJC5FCYQTPE6",
  "org_uei_num": "MJC5FCYQTPE6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Irvine",
  "perf_str_addr": "3206 Donald Bren Hall",
  "perf_city_name": "IRVINE",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "926970001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "47",
  "perf_st_cong_dist": "CA47",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "802300",
   "pgm_ele_name": "I-Corps"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": null
}