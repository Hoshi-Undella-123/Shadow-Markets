{
 "awd_id": "2513118",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Acceleration and Preconditioning Methods for Deep Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032922175",
 "po_email": "lzikatan@nsf.gov",
 "po_sign_block_name": "Ludmil T. Zikatanov",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2028-08-31",
 "tot_intn_awd_amt": 325000.0,
 "awd_amount": 325000.0,
 "awd_min_amd_letter_date": "2025-07-01",
 "awd_max_amd_letter_date": "2025-07-01",
 "awd_abstract_narration": "The remarkable progress of Artificial Intelligence (AI) in recent years is starting to greatly influence research across a wide range of disciplines. As Numerical Linear Algebra plays a crucial role in Deep Learning models, this trend presents unprecedented opportunities for experts in numerical analysis and linear algebra to contribute to ongoing AI research. This proposal represents a step toward capitalizing on this opportunity. The focus of the proposed work is not on applying AI to solve a specific problem, but rather on enhancing AI methods themselves by exploiting insights from numerical methods to optimize the Deep Learning process. This process is time-consuming, energy-intensive, resource-demanding, and overall very costly. Therefore, any improvements that can speed up the process are likely to have a significant impact. The investigators will leverage their experience in numerical methods to develop a number of techniques for accelerating the training of large AI models.\r\n\r\nThe project aims to develop techniques that exploit both accelerators and preconditioners to speed up iterative procedures used in training deep learning models.  The same combination of preconditioning and acceleration techniques is central to the effectiveness of iterative solution methods for linear systems. Acceleration methods such as Anderson/Pulay mixing or the Reduced Rank Extrapolation method, among others, have had immense success across various fields of science and engineering. However, in the context of deep learning, these methods face challenges, particularly since they were not developed for stochastic sequences common in deep learning.  The team will investigate the relationship between mini-batching, a technique used for sampling subfunctions in stochastic methods, and its impact on both the convergence speed and the accuracy of the resulting models. Simple diagonal preconditioning methods have already been incorporated into optimization techniques in deep learning.  The research team will explore more advanced preconditioning methods based on various approximations to the Fisher information matrix, a matrix that measures the amount of information that the observed data provides about the parameters. It has been shown that replacing the Hessian in second order methods by the Fisher matrix yields a more meaningful form of scaling of the variables, leading to better convergence and generalization. The investigating team will consider various methods for obtaining inexpensive approximations of the Fisher matrix and for combining them with accelerators. The proposed work is expected to benefit research in \"scientific machine learning\" by promoting participation of numerical linear algebra specialists in AI research. The PIs plan on several specific activities to promote the dissemination of knowledge in machine learning, such as writing a book on the topic of numerical methods in machine learning or offering tutorials and short courses. These activities will stimulate the interest of students in a field of increasing importance and that it will help with the immersion of those students from other areas, e.g., mathematics, into data-related disciplines.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuanzhe",
   "pi_last_name": "Xi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuanzhe Xi",
   "pi_email_addr": "yxi26@emory.edu",
   "nsf_id": "000787946",
   "pi_start_date": "2025-07-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Emory University",
  "inst_street_address": "201 DOWMAN DR NE",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4047272503",
  "inst_zip_code": "303221061",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "EMORY UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "S352L5PJLMP8"
 },
 "perf_inst": {
  "perf_inst_name": "Emory University",
  "perf_str_addr": "400 DOWMAN DR NE",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303224250",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  },
  {
   "pgm_ref_code": "079Z",
   "pgm_ref_txt": "Machine Learning Theory"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 325000.0
  }
 ],
 "por": null
}