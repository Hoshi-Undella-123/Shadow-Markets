{
 "awd_id": "2520553",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "FRR: Learning Object-Centric Representations from Human Demonstrations for Robot Manipulation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924420",
 "po_email": "cbethel@nsf.gov",
 "po_sign_block_name": "Cindy Bethel",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2028-08-31",
 "tot_intn_awd_amt": 399158.0,
 "awd_amount": 399158.0,
 "awd_min_amd_letter_date": "2025-07-31",
 "awd_max_amd_letter_date": "2025-07-31",
 "awd_abstract_narration": "Most robots today work in factories where people need to program every step of a task. This project explores how to make robots easier to use by helping them learn from watching people. This project uses artificial intelligence using reinforcement and imitation learning approaches. The goal is to teach robots how to see and handle objects by using video demonstrations of humans performing different actions. This is especially helpful with learning to grasp and move objects that have never been seen by the robot before. This way, robots can copy human tasks without needing detailed 3D models of every object they need to touch. If successful, the project could help bring robots into homes, hospitals, and other everyday places. It could also make robots more useful for people without the need for highly technical training. The project will support STEM education by building tools for teaching robotics and by including students in hands-on learning experiences.\r\n\r\n\r\nThis project develops a new learning framework that allows robots to understand and imitate object-based manipulation tasks. The research team will design models that enable robots to recognize and segment objects from visual structured memory. Next the robots will learn where and how to grasp objects by observing human contact patterns. The robot will then generate motion plans to manipulate those objects for specific goals. Lastly it will plan sequences of actions to complete multi-step tasks. The robots will use video data of people performing tasks in the real world to train the models. This is helpful as it is replicable and does not rely on physical interaction with a person to train the robot. This approach aims to help robots generalize and adapt to new tasks, environments, and platforms. It combines perception and learning from demonstration, for objects that have not been modeled or seen before. The research contributes to the fields of robot learning, object perception, and manipulation planning.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu",
   "pi_last_name": "Xiang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Xiang",
   "pi_email_addr": "yu.xiang@utdallas.edu",
   "nsf_id": "000870139",
   "pi_start_date": "2025-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Dallas",
  "inst_street_address": "800 WEST CAMPBELL RD.",
  "inst_street_address_2": "SP2.25",
  "inst_city_name": "RICHARDSON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9728832313",
  "inst_zip_code": "750803021",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "TX24",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT DALLAS",
  "org_prnt_uei_num": "",
  "org_uei_num": "EJCVPNN1WFS5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Dallas",
  "perf_str_addr": "800 WEST CAMPBELL RD.",
  "perf_city_name": "RICHARDSON",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "750803021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "TX24",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "144Y00",
   "pgm_ele_name": "FRR-Foundationl Rsrch Robotics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6840",
   "pgm_ref_txt": "ROBOTICS"
  },
  {
   "pgm_ref_code": "075Z",
   "pgm_ref_txt": "Artificial Intelligence (AI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 399158.0
  }
 ],
 "por": null
}