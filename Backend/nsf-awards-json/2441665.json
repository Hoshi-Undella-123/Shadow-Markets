{
 "awd_id": "2441665",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Unsupervised and Autonomous Reinforcement Learning of Skills",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032920000",
 "po_email": "eplaku@nsf.gov",
 "po_sign_block_name": "Erion Plaku",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2030-08-31",
 "tot_intn_awd_amt": 598210.0,
 "awd_amount": 476440.0,
 "awd_min_amd_letter_date": "2025-07-09",
 "awd_max_amd_letter_date": "2025-07-09",
 "awd_abstract_narration": "This project will study how artificial intelligence (AI) models can learn new tasks. Today, AI models are often taught to perform a task (e.g., controlling a self-driving car) by showing them examples of what a human would do. This project builds upon an area of research known as reinforcement learning, where AI models learn by trial and error, much like a dog might learn a trick by trying different behaviors and receiving a treat for the correct one. A major challenge in existing algorithms for trial-and-error learning is that complex tasks, such as assembling a house with a robotic arm, may require hundreds of small steps to complete. If the AI model receives feedback (a success or failure signal) only after completing the entire task, then it is difficult to figure out what went wrong in all the small intermediate steps. This project takes three key steps to address this challenge. First, the research will develop new algorithms to discover small, reusable skills. For a robotic arm, instead of teaching it how to build an entire house at once, the AI model might first learn a skill for stacking blocks, then another for sorting blocks, and so on. Importantly, the AI model discovers these skills by exploring and experimenting, without requiring human demonstrations or hand-written code. Once learned, these skills can be rapidly combined and adapted to solve new, more complex tasks. Second, the research will create new simulators and algorithms that leverage GPUs to significantly decrease the time required to learn a new task; tasks that previously took hours of computation time will now require just a few minutes. Third, the research will provide mathematical explanations that show when and why learning these skills will be effective. Taken together, this research will enable new algorithms for efficiently and robustly solving decision-making problems, with potential applications ranging from safely controlling self-driving cars to efficiently controlling factories.\r\n\r\nReinforcement learning (RL) has the potential to address many of the challenges in machine learning and AI today, enabling AI systems to reason about the consequences of their actions and to optimize their actions for long-term outcomes. In RL, an agent interacts with an environment, learning how to maximize a reward function through trial and error, often discovering strategies that are better and more robust than those designed by human experts. However, practical problems hinder the adoption of RL systems today: designing and implementing reward functions is difficult, and current RL algorithms require a prohibitively large amount of computational power. This research will leverage connections between self-supervised learning and RL to address these challenges, opening the door to new users and applications of RL. The first aim of this research is to develop a unified theory of existing methods in this space, starting from the observation that self-supervised learning, generative AI, and RL can all be defined in terms of information-theoretic and probabilistic quantities. The second aim is to build a new skill-learning algorithm that learns an exponentially large repertoire of skills by leveraging a novel hierarchical representation. This hierarchical representation will also enable new exploration strategies that utilize GPU-accelerated simulation. The third aim of the research is to use this repertoire of skills to develop new ways for users to interact with RL systems, to expand the planning horizon of RL methods, and to improve the exploration and robustness of RL agents. Because skills and their associated representations are learned with information-theoretic objectives, tools from probability theory can be used to analyze how best to use skills for these downstream applications. In parallel, this research will develop a curriculum of Jupyter notebooks for teaching RL to learners across the United States, helping to prepare the next generation of scientists and a future-ready AI workforce.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Eysenbach",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin Eysenbach",
   "pi_email_addr": "eysenbach@princeton.edu",
   "nsf_id": "000929903",
   "pi_start_date": "2025-07-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "619 Alexander Road",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085406000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002930DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 476440.0
  }
 ],
 "por": null
}