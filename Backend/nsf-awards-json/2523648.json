{
 "awd_id": "2523648",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: RI: Small: Unified Models for Sound Quality Assessment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2028-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2025-07-31",
 "awd_max_amd_letter_date": "2025-07-31",
 "awd_abstract_narration": "Advances in audio processing technologies like speech enhancement, audio compression, and hearing aids rely on automatic methods to evaluate whether processed audio sounds good to listeners. However, current computational approaches for assessing audio quality fail to match human perception, leading to systems that optimize for mathematical metrics rather than what actually sounds good to human ears. When engineers develop these technologies, existing evaluation metrics often disagree with human judgments, resulting in audio processing algorithms that may improve technical measurements while actually degrading the listening experience. This research will create computational tools that can automatically evaluate audio quality without requiring human listeners for every assessment, while maintaining strong agreement with human perception. The project develops new artificial intelligence models that learn to assess audio quality the way humans do, using novel machine learning training architectures and methodologies applied to human perceptual judgments across speech, music, and environmental sounds. These advancements will improve quality assessment for recorded speech, with direct applications in speech analysis and synthesis. This will ultimately lead to improvements in human language technologies such as speech enhancement, speaker extraction, and assistive hearing technologies which directly rely on perceptual quality assessments for improvements. They will also have a broader impact to audio technologies used in telecommunications, entertainment, medical devices, and consumer electronics by ensuring that automated systems optimize for genuine improvements during human listening experiences. The project will also support graduate student training in machine learning and audio processing, contributing to workforce development in these critical technical areas.\r\n\r\nThe technical approach builds on co-training architectures that simultaneously optimize full-reference, no-reference, and non-matching reference quality assessment models using shared embedding networks. Full-reference methods evaluate a degraded signal by comparing it to a clean reference version, while no-reference methods make the evaluation without regard to a clean version by modeling the statistics of clean audio. Recently introduced non-matching reference models provide an alternative that mitigates some limitations of both approaches by comparing a signal to a clean reference recording that contains different content. By co-training full-reference, no-reference, and non-matching reference architectures, the learned networks condition each other during training, leading to more robust models that correlate better with human perception. The research will also develop novel multi-task learning strategies that train across multiple objective quality measures (e.g., perceptual evaluation of speech quality (PESQ), signal-to-noise ratio (SNR), and scale-invariant signal-to-distortion ratio (SI-SDR)) while incorporating diverse subjective data including Mean Opinion Scores and pairwise comparisons. Analysis of the trained models will probe what acoustic, perceptual, and environmental information is captured in different layers of the learned representations. The project will validate these universal models through extensive evaluation on downstream audio processing tasks including speech synthesis, enhancement, speaker extraction, and music source separation. By creating loss functions and evaluation metrics that correlate strongly with human perception across diverse audio types, this research will enable the next generation of audio technologies that truly optimize for human auditory experience.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Donald",
   "pi_last_name": "Williamson",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Donald S Williamson",
   "pi_email_addr": "williamson.413@osu.edu",
   "nsf_id": "000737871",
   "pi_start_date": "2025-07-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "1960 KENNY RD",
  "perf_city_name": "COLUMBUS",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": null
}