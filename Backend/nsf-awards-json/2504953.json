{
 "awd_id": "2504953",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Research: CompCog: RI: Medium: Semantic Focusing: Controlling LM Interpretations for Human-Model Alignment",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922972",
 "po_email": "emiltsak@nsf.gov",
 "po_sign_block_name": "Eleni Miltsakaki",
 "awd_eff_date": "2025-09-01",
 "awd_exp_date": "2029-08-31",
 "tot_intn_awd_amt": 763741.0,
 "awd_amount": 763741.0,
 "awd_min_amd_letter_date": "2025-06-25",
 "awd_max_amd_letter_date": "2025-06-25",
 "awd_abstract_narration": "How do people derive meaning from sentences? In what situations does the language comprehension process break down? Artificial intelligence (AI) language models such as ChatGPT, which appear to understand and use language as proficiently as humans do, might seem poised to provide potential answers to this question\u2014answers that could not only enrich our scientific understanding, but also help address language processing deficits. But for AI systems to fully realize this potential, they need to process language in a similar way to humans. Many distinct lines of research show that this is not the case. One area where the discrepancy between humans and AI is particularly pronounced concerns temporary semantic ambiguity in language: cases where the first few words of the sentence are consistent with multiple interpretations, and only later in the sentence is it clear which of the interpretations is the correct one. Whereas human readers can encounter significant difficulty when they are required to change their interpretation of a sentence, AI models generally do not. The goal of this project is to better understand the reason for this misalignment between humans and AI models, and explore ways of modifying AI architectures to bring them more in line with how humans process language. In this project, the researchers will benchmark success in their model development by comparing how the models process language to how humans process language using a variety of psycholinguistic measurements. By better aligning human and AI language processing, this research will open up new directions to address long-standing limitations of current AI models, such as their need to train on far more data than human language learners do.\r\n\r\nIn more technical terms, this proposal explores the idea that one key difference between human and machine language processing is that humans: (i) entertain only a small number of semantic interpretations of the input at a time; and, (ii) treat incremental semantic inference as a key goal in language comprehension. This is pursued through three interrelated aims. First, the proposed work will explore the unexpectedly positive correlation between a model\u2019s perplexity and its ability to explain human reading times: put plainly, the better the model is at predicting the next word, the less similar its predictability estimates are to those of humans. Second, it explores whether the human-model misalignment can be alleviated by adopting semantic training objectives and leveraging causal intervention techniques to focus the model\u2019s internal representations of semantic context on a small number of possible interpretations of the input. Finally, human experiments will be conducted to test the predictions of the models on novel psycholinguistic stimuli, with the goal of determining if the proposed modifications successfully bring the models more in line with human language processing.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tal",
   "pi_last_name": "Linzen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tal Linzen",
   "pi_email_addr": "linzen@nyu.edu",
   "nsf_id": "000753386",
   "pi_start_date": "2025-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "New York University",
  "inst_street_address": "70 WASHINGTON SQ S",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2129982121",
  "inst_zip_code": "100121019",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "NY10",
  "org_lgl_bus_name": "NEW YORK UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NX9PXMKW5KW8"
 },
 "perf_inst": {
  "perf_inst_name": "New York University",
  "perf_str_addr": "70 WASHINGTON SQ S",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100121019",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "NY10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 763741.0
  }
 ],
 "por": null
}