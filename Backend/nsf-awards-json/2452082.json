{
 "awd_id": "2452082",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SHF: Small: Building scalable GPU simulation and efficient GPU memory management for large machine learning acceleration",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2025-10-01",
 "awd_exp_date": "2028-09-30",
 "tot_intn_awd_amt": 270000.0,
 "awd_amount": 270000.0,
 "awd_min_amd_letter_date": "2025-07-08",
 "awd_max_amd_letter_date": "2025-07-08",
 "awd_abstract_narration": "Recent advancements in large machine learning models have demonstrated that increasing the number of parameters enhances computational precision and unlocks capabilities once deemed unattainable. This trend is exemplified by the rapid growth in model sizes, for instance, GPT-3 contained 175 billion parameters, while GPT-4 reportedly utilizes up to 1.8 trillion. This trajectory is expected to continue in the foreseeable future. However, the explosive growth in model size presents two major challenges for computer architecture and systems research: prolonged simulation times, which can extend from several days to weeks for large-scale models, and infeasibility of deploying workloads on a single compute engine (e.g., a graphics processing unit (GPU)) due to limited on-device memory capacity. To address these challenges, this project proposes the development of scalable simulation techniques and advanced memory management strategies tailored for large-scale machine learning workloads on GPUs. Unlike existing application-agnostic approaches, this research will leverage the distinctive data access patterns and value distributions of modern machine learning models to enable more efficient memory compression and more accurate simulation acceleration. While the primary focus will be on emerging machine learning models, the broader objective is to advance GPU computing to better accommodate any big data workload constrained by memory limitations. This will facilitate faster and broader adoption of GPUs across diverse computing domains, driving continued innovation in computational science. The outcomes of this research will be integrated into both new and existing undergraduate and graduate curricula, as well as K-12 outreach initiatives, fostering a deeper understanding of cutting-edge computing technologies across educational levels.\r\n\r\nThis project would answer two research questions: how to simulate large machine learning computing and how to utilize GPU local memory better when the memory is oversubscribed. While large-scale simulation and memory management have been widely studied, most existing approaches fail to capture the unique architectural characteristics of GPU computing and the specific behaviors of emerging machine learning workloads. Rather than relying on application-agnostic or user-dependent sampling techniques, this research will exploit the distinctive compute and memory access patterns inherent to machine learning models. The first thrust will research efficient simulator acceleration methodology by leveraging the fact that machine learning models are typically executed with highly optimized library functions. These library functions tend to have similar architectural behaviors depending on the operational and data size characteristics. The project will identify representative sample kernels whose performance can be extrapolated to other similar kernels, thereby significantly reducing simulation overhead. By leveraging characteristics of the library functions, the second thrust will explore efficient memory expansion and compression strategies such as dynamic memory prefetching and eviction policies to mitigate the effects of memory oversubscription. The second thrust will develop novel quantization techniques that take advantage of the unique value distributions of weights and gradients within individual tensors. Unlike tensor-oblivious methods, this targeted approach aims to reduce memory footprint more effectively while preserving model accuracy.\r\n\r\nThis award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Hyesoon",
   "pi_last_name": "Kim",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hyesoon Kim",
   "pi_email_addr": "hyesoon@cc.gatech.edu",
   "nsf_id": "000084212",
   "pi_start_date": "2025-07-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Georgia Tech Research Corporation",
  "inst_street_address": "926 DALNEY ST NW",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4048944819",
  "inst_zip_code": "303186395",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "GEORGIA TECH RESEARCH CORP",
  "org_prnt_uei_num": "EMW9FC8J3HN4",
  "org_uei_num": "EMW9FC8J3HN4"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute Of Technology",
  "perf_str_addr": "225 North Avenue",
  "perf_city_name": "ATLANTA",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303320002",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "",
   "app_name": "",
   "app_symb_id": "",
   "fund_code": "01002526DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2025,
   "fund_oblg_amt": 270000.0
  }
 ],
 "por": null
}